{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d400e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import os\n",
    "import pandas as pd\n",
    "import wfdb.processing as wp\n",
    "import numpy as np\n",
    "import pickle\n",
    "from biosppy.signals import ecg, tools\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch import nn, optim\n",
    "\n",
    "import pytorch_model_summary\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler as mms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIbLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f0bb68",
   "metadata": {},
   "source": [
    "# Data Load (using WFDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c48d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = \"./mit_pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bece0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_extract(input_path):\n",
    "    records = open(input_path+\"RECORDS\",\"r\")\n",
    "    records_list = []\n",
    "    for l in records:\n",
    "        l = l.rstrip()\n",
    "        records_list.append(l)\n",
    "    records.close()\n",
    "    return records_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e434c83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '111', '112', '113', '114', '115', '116', '117', '118', '119', '121', '122', '123', '124', '200', '201', '202', '203', '205', '207', '208', '209', '210', '212', '213', '214', '215', '217', '219', '220', '221', '222', '223', '228', '230', '231', '232', '233', '234']\n"
     ]
    }
   ],
   "source": [
    "#data extract from Physionet\n",
    "\n",
    "input_path = \"../physionet/mit-bih_arr/1.0.0/\"\n",
    "records = open(input_path+\"RECORDS\",\"r\")\n",
    "records_list = []\n",
    "for l in records:\n",
    "    l = l.rstrip()\n",
    "    records_list.append(l)\n",
    "records.close()\n",
    "print(records_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f50a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMAL_ANN = ['N', 'L', 'R', 'e', 'j']\n",
    "SUPRA_ANN = ['A', 'a', 'J', 'S']\n",
    "VENTRI_ANN = ['V', 'E']\n",
    "FUSION_ANN = ['F']\n",
    "UNCLASS_ANN = ['/', 'f', 'Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f2c12bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'record_sig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hb \u001b[38;5;241m=\u001b[39m ecg\u001b[38;5;241m.\u001b[39mextract_heartbeats(signal\u001b[38;5;241m=\u001b[39m\u001b[43mrecord_sig\u001b[49m[\u001b[38;5;241m0\u001b[39m][:,\u001b[38;5;241m0\u001b[39m], rpeaks\u001b[38;5;241m=\u001b[39mrecord_ann, sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m360\u001b[39m,before\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,after\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'record_sig' is not defined"
     ]
    }
   ],
   "source": [
    "hb = ecg.extract_heartbeats(signal=record_sig[0][:,0], rpeaks=record_ann, sampling_rate=360,before=0.3,after=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce7e2c",
   "metadata": {},
   "source": [
    "rpeaks_ch = ecg.christov_segmenter(signal=record_sig[0][:,0], sampling_rate=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9fdcdc",
   "metadata": {},
   "source": [
    "rpeaks_ha = ecg.hamilton_segmenter(signal=record_sig[0][:,0], sampling_rate=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be49a06",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "comp = ecg.compare_segmentation(rpeaks_ha[\"rpeaks\"],record_ann,sampling_rate=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3cde55",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "count = 0\n",
    "for i in comp[\"match\"]:\n",
    "    if i == count:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Missing match I,\",i,count)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd7c2e5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "len(beat[\"Beats\"]),len(beat[\"rpeaks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "913c09d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'record_ann_sym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m beat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mrecord_ann_sym\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'record_ann_sym' is not defined"
     ]
    }
   ],
   "source": [
    "beat[\"symbol\"]=record_ann_sym[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce713ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'beat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler \u001b[38;5;28;01mas\u001b[39;00m mms\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbeat\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[1;32m      5\u001b[0m ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'beat' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler as mms\n",
    "beat[\"\"]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 7))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_title(\"Heart Beat\")\n",
    "ax.plot(beat[\"Beats\"][1080])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2afa06a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 100\n",
      "Beats, Rpeaks Len,  2273 2273 2273\n",
      "320 320\n",
      "Record 101\n",
      "Beats, Rpeaks Len,  1873 1873 1873\n",
      "320 320\n",
      "Record 102\n",
      "Beats, Rpeaks Len,  2191 2191 2191\n",
      "320 320\n",
      "Record 103\n",
      "Beats, Rpeaks Len,  2090 2090 2090\n",
      "320 320\n",
      "Record 104\n",
      "Beats, Rpeaks Len,  2310 2310 2310\n",
      "320 320\n",
      "Record 105\n",
      "Beats, Rpeaks Len,  2690 2690 2690\n",
      "320 320\n",
      "Record 106\n",
      "Beats, Rpeaks Len,  2097 2097 2097\n",
      "320 320\n",
      "Record 107\n",
      "Beats, Rpeaks Len,  2139 2139 2139\n",
      "320 320\n",
      "Record 108\n",
      "Beats, Rpeaks Len,  1823 1823 1823\n",
      "320 320\n",
      "Record 109\n",
      "Beats, Rpeaks Len,  2534 2534 2534\n",
      "320 320\n",
      "Record 111\n",
      "Beats, Rpeaks Len,  2132 2132 2132\n",
      "320 320\n",
      "Record 112\n",
      "Beats, Rpeaks Len,  2549 2549 2549\n",
      "320 320\n",
      "Record 113\n",
      "Beats, Rpeaks Len,  1795 1795 1795\n",
      "320 320\n",
      "Record 114\n",
      "Beats, Rpeaks Len,  1889 1889 1889\n",
      "320 320\n",
      "Record 115\n",
      "Beats, Rpeaks Len,  1961 1961 1961\n",
      "320 320\n",
      "Record 116\n",
      "Beats, Rpeaks Len,  2420 2420 2420\n",
      "320 320\n",
      "Record 117\n",
      "Beats, Rpeaks Len,  1538 1538 1538\n",
      "320 320\n",
      "Record 118\n",
      "Beats, Rpeaks Len,  2300 2300 2300\n",
      "320 320\n",
      "Record 119\n",
      "Beats, Rpeaks Len,  2093 2093 2093\n",
      "320 320\n",
      "Record 121\n",
      "Beats, Rpeaks Len,  1875 1875 1875\n",
      "320 320\n",
      "Record 122\n",
      "Beats, Rpeaks Len,  2478 2478 2478\n",
      "320 320\n",
      "Record 123\n",
      "Beats, Rpeaks Len,  1518 1518 1518\n",
      "320 320\n",
      "Record 124\n",
      "Beats, Rpeaks Len,  1633 1633 1633\n",
      "320 320\n",
      "Record 200\n",
      "Beats, Rpeaks Len,  2791 2791 2791\n",
      "320 320\n",
      "Record 201\n",
      "Beats, Rpeaks Len,  2038 2038 2038\n",
      "320 320\n",
      "Record 202\n",
      "Beats, Rpeaks Len,  2145 2145 2145\n",
      "320 320\n",
      "Record 203\n",
      "Beats, Rpeaks Len,  3107 3107 3107\n",
      "320 320\n",
      "Record 205\n",
      "Beats, Rpeaks Len,  2671 2671 2671\n",
      "320 320\n",
      "Record 207\n",
      "Beats, Rpeaks Len,  2384 2384 2384\n",
      "320 320\n",
      "Record 208\n",
      "Beats, Rpeaks Len,  3039 3039 3039\n",
      "320 320\n",
      "Record 209\n",
      "Beats, Rpeaks Len,  3051 3051 3051\n",
      "320 320\n",
      "Record 210\n",
      "Beats, Rpeaks Len,  2684 2684 2684\n",
      "320 320\n",
      "Record 212\n",
      "Beats, Rpeaks Len,  2762 2762 2762\n",
      "320 320\n",
      "Record 213\n",
      "Beats, Rpeaks Len,  3293 3293 3293\n",
      "320 320\n",
      "Record 214\n",
      "Beats, Rpeaks Len,  2296 2296 2296\n",
      "320 320\n",
      "Record 215\n",
      "Beats, Rpeaks Len,  3399 3399 3399\n",
      "320 320\n",
      "Record 217\n",
      "Beats, Rpeaks Len,  2279 2279 2279\n",
      "320 320\n",
      "Record 219\n",
      "Beats, Rpeaks Len,  2311 2311 2311\n",
      "320 320\n",
      "Record 220\n",
      "Beats, Rpeaks Len,  2068 2068 2068\n",
      "320 320\n",
      "Record 221\n",
      "Beats, Rpeaks Len,  2461 2461 2461\n",
      "320 320\n",
      "Record 222\n",
      "Beats, Rpeaks Len,  2633 2633 2633\n",
      "320 320\n",
      "Record 223\n",
      "Beats, Rpeaks Len,  2642 2642 2642\n",
      "320 320\n",
      "Record 228\n",
      "Beats, Rpeaks Len,  2140 2140 2140\n",
      "320 320\n",
      "Record 230\n",
      "Beats, Rpeaks Len,  2465 2465 2465\n",
      "320 320\n",
      "Record 231\n",
      "Beats, Rpeaks Len,  2010 2010 2010\n",
      "320 320\n",
      "Record 232\n",
      "Beats, Rpeaks Len,  1815 1815 1815\n",
      "320 320\n",
      "Record 233\n",
      "Beats, Rpeaks Len,  3151 3151 3151\n",
      "320 320\n",
      "Record 234\n",
      "Beats, Rpeaks Len,  2763 2763 2763\n",
      "320 320\n"
     ]
    }
   ],
   "source": [
    "window_size = 320\n",
    "def extract_from_sbj(idx):\n",
    "    record_sig = wfdb.rdsamp(input_path+records_list[idx], channels=[0], sampfrom=0)\n",
    "    record_ann = list(wfdb.rdann(input_path+records_list[idx],\"atr\", sampfrom=0).sample)[1:]\n",
    "    record_ann_sym = list(wfdb.rdann(input_path+records_list[idx], \"atr\", sampfrom=0).symbol)[1:]\n",
    "    interval = wp.ann2rr(input_path+records_list[idx], \"atr\", as_array=True)\n",
    "    \n",
    "    rpeaks_ch = ecg.christov_segmenter(signal=record_sig[0][:,0], sampling_rate=360)\n",
    "    rpeaks_ha = ecg.hamilton_segmenter(signal=record_sig[0][:,0], sampling_rate=360)\n",
    "    \n",
    "    #h_beat = ecg.extract_heartbeats(signal=record_sig[0][:,0], rpeaks=rpeaks_ha, sampling_rate=360)\n",
    "    NORMAL_ANN = ['N', 'L', 'R', 'e', 'j']\n",
    "    SUPRA_ANN = ['A', 'a', 'J', 'S']\n",
    "    VENTRI_ANN = ['V', 'E']\n",
    "    FUSION_ANN = ['F']\n",
    "    UNCLASS_ANN = ['/', 'f', 'Q']\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    0 : N, 1 : S, 2 : V, 3 : F, 4 : Q, 5: \"/\"\n",
    "    \"\"\"\n",
    "    for i,sym in enumerate(record_ann_sym):\n",
    "        if sym in NORMAL_ANN:\n",
    "            record_ann_sym[i] = 0\n",
    "        elif sym in SUPRA_ANN:\n",
    "            record_ann_sym[i] = 1\n",
    "        elif sym in VENTRI_ANN:\n",
    "            record_ann_sym[i] = 2\n",
    "        elif sym in FUSION_ANN:\n",
    "            record_ann_sym[i] = 3\n",
    "        elif sym in UNCLASS_ANN:\n",
    "            record_ann_sym[i] = 4\n",
    "        else:\n",
    "            record_ann_sym[i] = 5\n",
    "    \n",
    "    def beat_Padding(beat,w_size):\n",
    "        beat_len = len(beat)\n",
    "        if beat_len <= w_size:\n",
    "            pd_size = (w_size - beat_len)//2\n",
    "        pad_beat = np.pad(beat, (pd_size, pd_size), \"constant\", constant_values=0)\n",
    "        \n",
    "        if len(pad_beat) < w_size:\n",
    "            pad_beat = np.pad(pad_beat, (0, w_size - len(pad_beat)), \"constant\", constant_values=0)\n",
    "        elif len(pad_beat) > w_size:\n",
    "            pad_beat = pad_beat[0:-(len(pad_beat)-w_size)]\n",
    "        \n",
    "        \n",
    "        return pad_beat\n",
    "    \n",
    "    def beat_extraction(signal,rpeaks): # Peak를 기점으로 앞뒤로 0.3초 정도의 window를 가지는 Beat들\n",
    "        beat = []\n",
    "        for i,rp in enumerate(rpeaks):\n",
    "            if i == 0: # 0.3초 정도의 window\n",
    "                start = int(rp-(0.3*360))\n",
    "                if start < 0:\n",
    "                    start = 0\n",
    "                end = int(rp+(0.3*360))\n",
    "                sig_beat = beat_Padding(signal[start:end], window_size)\n",
    "                beat.append(sig_beat)\n",
    "                continue\n",
    "            \n",
    "            # 양쪽으로 30% 정도 남겨서 짜르기\n",
    "            \n",
    "            start = int(rp-(0.3 * 360))\n",
    "            end = int(rp+(0.3 * 360))\n",
    "            \n",
    "            if end > len(signal):\n",
    "                end = len(signal-1)\n",
    "                \n",
    "            # Beat Padding\n",
    "            sig_beat = beat_Padding(signal[start:end], window_size)\n",
    "            beat.append(sig_beat)\n",
    "\n",
    "        return {\"Beats\":beat,\"rpeaks\":rpeaks, \"symbol\":record_ann}\n",
    "    \n",
    "    beat = beat_extraction(record_sig[0][:,0], record_ann)\n",
    "    \n",
    "    print(\"Beats, Rpeaks Len, \",len(beat[\"Beats\"]),len(beat[\"rpeaks\"]), len(record_ann))\n",
    "    \n",
    "    beat[\"symbol\"]=record_ann_sym[:]\n",
    "    max_len = -1\n",
    "    min_len = 9999\n",
    "    for i,datas in enumerate(zip(beat[\"rpeaks\"],beat[\"symbol\"])):\n",
    "        max_len = max(max_len, len(beat[\"Beats\"][i]))\n",
    "        min_len = min(min_len, len(beat[\"Beats\"][i]))\n",
    "    return max_len, min_len, beat\n",
    "\n",
    "datas = {}\n",
    "\n",
    "for i,rec in enumerate(records_list):\n",
    "    print(\"Record\",rec)\n",
    "    maxs, mins, beats = extract_from_sbj(i)\n",
    "    datas[rec]=beats\n",
    "    print(maxs,mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6674473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = \"./pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a07ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pickle File\n",
    "\n",
    "import pickle\n",
    "\n",
    "for rec in records_list:\n",
    "    pickle_data = pickle_path + \"/\" + rec + \".pkl\"\n",
    "    with open(pickle_data,\"wb\") as f:\n",
    "        pickle.dump(datas[rec], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e0a1a",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da3046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation of Electrocardiogram Heartbeat by CNN and GRU (not Use)\n",
    "class LFEM(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size):\n",
    "        super(LFEM, self).__init__()\n",
    "        \n",
    "        self.lfem_seq = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(out_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lfem_seq(x)\n",
    "\n",
    "class LFEM_Stack(nn.Module):\n",
    "    def __init__(self, num_layer=6):\n",
    "        super(LFEM_Stack, self).__init__()\n",
    "        \n",
    "        self.num_layer = num_layer\n",
    "        self.lfem_list = []\n",
    "        layer_size=[64,64,128,128,256,256]\n",
    "        for i in range(num_layer-1):\n",
    "            self.lfem_list.append(LFEM(in_channel=layer_size[i], out_channel=layer_size[i+1], kernel_size=3))\n",
    "        self.lfem_list.append(LFEM(in_channel=layer_size[5], out_channel=layer_size[5], kernel_size=3))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layer):\n",
    "            x = self.lfem_list[i](x)\n",
    "            \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7df9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_gru(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, batch, hidden_size):\n",
    "        super(conv_gru, self).__init__()\n",
    "        \n",
    "        #input Layer\n",
    "        self.input_Seq1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channel, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.input_Seq2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.input_Seq3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.input_Seq4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "#         self.lfem_module = LFEM_Stack(6)\n",
    "        # reshape\n",
    "        # dropout\n",
    "        # gru\n",
    "        # dense 1\n",
    "        # dense 2\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.gru = nn.GRU(input_size=320,batch_first=True, hidden_size=hidden_size ,num_layers=2, bidirectional=False)\n",
    "        \n",
    "        self.dense_Seq = nn.Sequential(\n",
    "            nn.Linear(256*hidden_size, 256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(256,out_channel)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.input_Seq1(x)\n",
    "        x = self.input_Seq2(x)\n",
    "        x = self.input_Seq3(x)\n",
    "        x = self.input_Seq4(x)\n",
    "        x = self.dropout(x)\n",
    "        x, _ = self.gru(x)\n",
    "        x = x.reshape(x.size(0),-1)\n",
    "        print(x.shape)\n",
    "        x = self.dense_Seq(x)\n",
    "#         x = self.dense1(x)\n",
    "#         x = self.dense2(x)\n",
    "        \n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e57be0a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv_gru(\n",
       "  (input_Seq1): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (input_Seq2): Sequential(\n",
       "    (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (input_Seq3): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (input_Seq4): Sequential(\n",
       "    (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (gru): GRU(320, 256, num_layers=2, batch_first=True)\n",
       "  (dense_Seq): Sequential(\n",
       "    (0): Linear(in_features=65536, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (2): Linear(in_features=256, out_features=5, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = conv_gru(in_channel=1,out_channel=5,batch=32,hidden_size=256)\n",
    "model.to(device)\n",
    "# torch.shape : (batch, Channel, W(1채널의경우 1), H)\n",
    "# print(\"Model output Shape, \",model(torch.randn(32,1,320)).shape) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d08c8d0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65536])\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)         Input Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Conv1d-1        [32, 1, 320]             128             128\n",
      "     BatchNorm1d-2       [32, 32, 320]              64              64\n",
      "            ReLU-3       [32, 32, 320]               0               0\n",
      "          Conv1d-4       [32, 32, 320]           6,208           6,208\n",
      "     BatchNorm1d-5       [32, 64, 320]             128             128\n",
      "            ReLU-6       [32, 64, 320]               0               0\n",
      "          Conv1d-7       [32, 64, 320]          24,704          24,704\n",
      "     BatchNorm1d-8      [32, 128, 320]             256             256\n",
      "            ReLU-9      [32, 128, 320]               0               0\n",
      "         Conv1d-10      [32, 128, 320]          98,560          98,560\n",
      "    BatchNorm1d-11      [32, 256, 320]             512             512\n",
      "           ReLU-12      [32, 256, 320]               0               0\n",
      "        Dropout-13      [32, 256, 320]               0               0\n",
      "            GRU-14      [32, 256, 320]         838,656         838,656\n",
      "         Linear-15         [32, 65536]      16,777,472      16,777,472\n",
      "      LeakyReLU-16           [32, 256]               0               0\n",
      "         Linear-17           [32, 256]           1,285           1,285\n",
      "        Softmax-18             [32, 5]               0               0\n",
      "=======================================================================\n",
      "Total params: 17,747,973\n",
      "Trainable params: 17,747,973\n",
      "Non-trainable params: 0\n",
      "Batch size: 32\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "======================================== Hierarchical Summary ========================================\n",
      "\n",
      "conv_gru(\n",
      "  (input_Seq1): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,)), 128 params\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 64 params\n",
      "    (2): ReLU(), 0 params\n",
      "  ), 192 params\n",
      "  (input_Seq2): Sequential(\n",
      "    (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,)), 6,208 params\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 128 params\n",
      "    (2): ReLU(), 0 params\n",
      "  ), 6,336 params\n",
      "  (input_Seq3): Sequential(\n",
      "    (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,)), 24,704 params\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 256 params\n",
      "    (2): ReLU(), 0 params\n",
      "  ), 24,960 params\n",
      "  (input_Seq4): Sequential(\n",
      "    (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,)), 98,560 params\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 512 params\n",
      "    (2): ReLU(), 0 params\n",
      "  ), 99,072 params\n",
      "  (dropout): Dropout(p=0.3, inplace=False), 0 params\n",
      "  (gru): GRU(320, 256, num_layers=2, batch_first=True), 838,656 params\n",
      "  (dense_Seq): Sequential(\n",
      "    (0): Linear(in_features=65536, out_features=256, bias=True), 16,777,472 params\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace=True), 0 params\n",
      "    (2): Linear(in_features=256, out_features=5, bias=True), 1,285 params\n",
      "  ), 16,778,757 params\n",
      "  (softmax): Softmax(dim=1), 0 params\n",
      "), 17,747,973 params\n",
      "\n",
      "\n",
      "======================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pytorch_model_summary.summary(model,torch.randn(32,1,320).cuda(), show_input=True, batch_size=32, show_hierarchical=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f528add",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972657ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pickle Data Load\n",
    "\n",
    "def load_data():\n",
    "    pkl_path = \"./pickle/\"\n",
    "\n",
    "    data_x, data_y = [] ,[] \n",
    "    for pkl_file in os.listdir(pkl_path):\n",
    "        with open(pkl_path+pkl_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            for i,d in enumerate(data[\"Beats\"]):\n",
    "                data_x.append(d)\n",
    "                data_y.append(data[\"symbol\"][i])\n",
    "#             print(torch.tensor(data[\"Beats\"]).shape)\n",
    "#             print(torch.tensor(data[\"symbol\"]).shape)\n",
    "    return torch.tensor(data_x), torch.tensor(data_y)\n",
    "\n",
    "x_data, y_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e021e5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112599, 320])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c4a339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112599])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30cd3d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112599\n",
      "22519\n",
      "90080\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.utils.data.TensorDataset(x_data,y_data)\n",
    "train_len = x_data.shape[0]\n",
    "print(train_len)\n",
    "val_len = int(train_len * 0.2)\n",
    "print(val_len)\n",
    "train_len -= val_len\n",
    "print(train_len)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_data, [train_len, val_len])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e293fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "epochs = 50\n",
    "torch.manual_seed(1234)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = conv_gru(in_channel=1,out_channel=5,batch=32,hidden_size=256)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc, best_epoch = 0, 0\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58635ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def main():\n",
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d9e3335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data 확인\n",
    "# dataiter = iter(train_loader)\n",
    "# sig, lab = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b873a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de2063ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                     | 0/2815 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 320]) torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 12\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat(), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    train_bar = tqdm(train_loader)\n",
    "    for step, (x,y) in enumerate(train_bar):\n",
    "        \n",
    "        x = x.unsqueeze(dim=1)\n",
    "        print(x.shape, y.shape)\n",
    "        x, y = x.to(device).float(), y.to(device)\n",
    "        print(y)\n",
    "        model.train()\n",
    "        logits = model(x)\n",
    "        print(logits.shape)\n",
    "        loss = criterion(logits, y)\n",
    "        print(loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step() \n",
    "        \n",
    "        train_bar.desc = \"Train Epoch[{}/{}] loss: {:.3f}\".format(ep+1, epochs, loss)\n",
    "        global_step +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3adca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
