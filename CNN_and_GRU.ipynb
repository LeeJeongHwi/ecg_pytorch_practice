{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2a3165",
   "metadata": {},
   "source": [
    "## Paper : Interpretation of Electrocardiogram Heartbeat by CNN and GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "633168be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import os\n",
    "import pandas as pd\n",
    "import wfdb.processing as wp\n",
    "import numpy as np\n",
    "import pickle\n",
    "from biosppy.signals import ecg, tools\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch import nn, optim\n",
    "\n",
    "import pytorch_model_summary\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler as mms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIbLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7d6995db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation of Electrocardiogram Heartbeat by CNN and GRU (not Use)\n",
    "class LFEM(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size):\n",
    "        super(LFEM, self).__init__()\n",
    "        \n",
    "        self.lfem_seq = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.Conv1d(in_channels=out_channel, out_channels=out_channel, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(out_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lfem_seq(x)\n",
    "\n",
    "class LFEM_Stack(nn.Module):\n",
    "    def __init__(self, num_layer=6):\n",
    "        super(LFEM_Stack, self).__init__()\n",
    "        \n",
    "        self.num_layer = num_layer\n",
    "        self.lfem_list = []\n",
    "        layer_size=[64,64,128,128,256,256]\n",
    "        for i in range(num_layer-1):\n",
    "            self.lfem_list.append(LFEM(in_channel=layer_size[i], out_channel=layer_size[i+1], kernel_size=3))\n",
    "        self.lfem_list.append(LFEM(in_channel=layer_size[5], out_channel=layer_size[5], kernel_size=3))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layer):\n",
    "            x = self.lfem_list[i](x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ab9d0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_gru(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, batch, hidden_size):\n",
    "        super(conv_gru, self).__init__()\n",
    "        \n",
    "        self.input_seq = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channel,out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.LFEM1 = LFEM(32,64,3)\n",
    "        self.LFEM2 = LFEM(64,64,3)\n",
    "        self.LFEM3 = LFEM(64,128,3)\n",
    "        self.LFEM4 = LFEM(128,128,3)\n",
    "        self.LFEM5 = LFEM(128,256,3)\n",
    "        self.LFEM6 = LFEM(256,256,3)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=1280 ,batch_first=True, hidden_size=hidden_size, num_layers=2, bidirectional=False)\n",
    "        self.dense_seq = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(256, out_channel)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = self.input_seq(x)\n",
    "        x = self.LFEM1(x)\n",
    "        x = self.LFEM2(x)\n",
    "        x = self.LFEM3(x)\n",
    "        x = self.LFEM4(x)\n",
    "        x = self.LFEM5(x)\n",
    "        x = self.LFEM6(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.reshape(x.size(0),-1)\n",
    "        x = x.unsqueeze(1)\n",
    "        x, _ = self.gru(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.dense_seq(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5caffe0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv_gru(\n",
       "  (input_seq): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (LFEM1): LFEM(\n",
       "    (lfem_seq): Sequential(\n",
       "      (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (LFEM2): LFEM(\n",
       "    (lfem_seq): Sequential(\n",
       "      (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (LFEM3): LFEM(\n",
       "    (lfem_seq): Sequential(\n",
       "      (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (LFEM4): LFEM(\n",
       "    (lfem_seq): Sequential(\n",
       "      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (LFEM5): LFEM(\n",
       "    (lfem_seq): Sequential(\n",
       "      (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (LFEM6): LFEM(\n",
       "    (lfem_seq): Sequential(\n",
       "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (gru): GRU(1280, 256, num_layers=2, batch_first=True)\n",
       "  (dense_seq): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (2): Linear(in_features=256, out_features=6, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = conv_gru(in_channel=1, out_channel=6, batch=32, hidden_size=256)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b606c10c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)         Input Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Conv1d-1        [32, 1, 320]             128             128\n",
      "     BatchNorm1d-2       [32, 32, 320]              64              64\n",
      "            ReLU-3       [32, 32, 320]               0               0\n",
      "            LFEM-4       [32, 32, 320]          18,688          18,688\n",
      "            LFEM-5       [32, 64, 160]          24,832          24,832\n",
      "            LFEM-6        [32, 64, 80]          74,240          74,240\n",
      "            LFEM-7       [32, 128, 40]          98,816          98,816\n",
      "            LFEM-8       [32, 128, 20]         295,936         295,936\n",
      "            LFEM-9       [32, 256, 10]         394,240         394,240\n",
      "        Dropout-10        [32, 256, 5]               0               0\n",
      "            GRU-11       [32, 1, 1280]       1,575,936       1,575,936\n",
      "         Linear-12           [32, 256]          65,792          65,792\n",
      "      LeakyReLU-13           [32, 256]               0               0\n",
      "         Linear-14           [32, 256]           1,542           1,542\n",
      "        Softmax-15             [32, 6]               0               0\n",
      "=======================================================================\n",
      "Total params: 2,550,214\n",
      "Trainable params: 2,550,214\n",
      "Non-trainable params: 0\n",
      "Batch size: 32\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================= Hierarchical Summary =========================================\n",
      "\n",
      "conv_gru(\n",
      "  (input_seq): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,)), 128 params\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 64 params\n",
      "    (2): ReLU(), 0 params\n",
      "  ), 192 params\n",
      "  (LFEM1): LFEM(\n",
      "    (lfem_seq): Sequential(\n",
      "      (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,)), 6,208 params\n",
      "      (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,)), 12,352 params\n",
      "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 128 params\n",
      "      (3): ReLU(), 0 params\n",
      "      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), 0 params\n",
      "    ), 18,688 params\n",
      "  ), 18,688 params\n",
      "  (LFEM2): LFEM(\n",
      "    (lfem_seq): Sequential(\n",
      "      (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,)), 12,352 params\n",
      "      (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,)), 12,352 params\n",
      "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 128 params\n",
      "      (3): ReLU(), 0 params\n",
      "      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), 0 params\n",
      "    ), 24,832 params\n",
      "  ), 24,832 params\n",
      "  (LFEM3): LFEM(\n",
      "    (lfem_seq): Sequential(\n",
      "      (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,)), 24,704 params\n",
      "      (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,)), 49,280 params\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 256 params\n",
      "      (3): ReLU(), 0 params\n",
      "      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), 0 params\n",
      "    ), 74,240 params\n",
      "  ), 74,240 params\n",
      "  (LFEM4): LFEM(\n",
      "    (lfem_seq): Sequential(\n",
      "      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,)), 49,280 params\n",
      "      (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,)), 49,280 params\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 256 params\n",
      "      (3): ReLU(), 0 params\n",
      "      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), 0 params\n",
      "    ), 98,816 params\n",
      "  ), 98,816 params\n",
      "  (LFEM5): LFEM(\n",
      "    (lfem_seq): Sequential(\n",
      "      (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,)), 98,560 params\n",
      "      (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,)), 196,864 params\n",
      "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 512 params\n",
      "      (3): ReLU(), 0 params\n",
      "      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), 0 params\n",
      "    ), 295,936 params\n",
      "  ), 295,936 params\n",
      "  (LFEM6): LFEM(\n",
      "    (lfem_seq): Sequential(\n",
      "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,)), 196,864 params\n",
      "      (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,)), 196,864 params\n",
      "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 512 params\n",
      "      (3): ReLU(), 0 params\n",
      "      (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), 0 params\n",
      "    ), 394,240 params\n",
      "  ), 394,240 params\n",
      "  (dropout): Dropout(p=0.3, inplace=False), 0 params\n",
      "  (gru): GRU(1280, 256, num_layers=2, batch_first=True), 1,575,936 params\n",
      "  (dense_seq): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True), 65,792 params\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace=True), 0 params\n",
      "    (2): Linear(in_features=256, out_features=6, bias=True), 1,542 params\n",
      "  ), 67,334 params\n",
      "  (softmax): Softmax(dim=1), 0 params\n",
      "), 2,550,214 params\n",
      "\n",
      "\n",
      "========================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pytorch_model_summary.summary(model,torch.randn(32,1,320).cuda(), show_input=True, batch_size=32, show_hierarchical=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a3d9e692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112599, 320]) torch.Size([112599])\n",
      "90080 22519\n"
     ]
    }
   ],
   "source": [
    "# Pickle Data Load\n",
    "\n",
    "def load_data():\n",
    "    pkl_path = \"./pickle/\"\n",
    "\n",
    "    data_x, data_y = [] ,[] \n",
    "    for pkl_file in os.listdir(pkl_path):\n",
    "        if os.path.isdir(pkl_path+pkl_file):\n",
    "            continue\n",
    "        with open(pkl_path+pkl_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            for i,d in enumerate(data[\"Beats\"]):\n",
    "                data_x.append(d)\n",
    "                data_y.append(data[\"symbol\"][i])\n",
    "#             print(torch.tensor(data[\"Beats\"]).shape)\n",
    "#             print(torch.tensor(data[\"symbol\"]).shape)\n",
    "    return torch.tensor(data_x), torch.tensor(data_y)\n",
    "\n",
    "x_data, y_data = load_data()\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(x_data,y_data)\n",
    "train_len = x_data.shape[0]\n",
    "# print(train_len)\n",
    "val_len = int(train_len * 0.2)\n",
    "# print(val_len)\n",
    "train_len -= val_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_data, [train_len, val_len])\n",
    "print(train_len, val_len)\n",
    "\n",
    "lr = 1e-5\n",
    "epochs = 30\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = conv_gru(in_channel=1,out_channel=6,batch=256,hidden_size=256)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc, best_epoch = 0, 0\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "22a420f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112599, 320])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6e436479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112599])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "bb2fc82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112599\n",
      "22519\n",
      "90080\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.utils.data.TensorDataset(x_data,y_data)\n",
    "train_len = x_data.shape[0]\n",
    "print(train_len)\n",
    "val_len = int(train_len * 0.2)\n",
    "print(val_len)\n",
    "train_len -= val_len\n",
    "print(train_len)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_data, [train_len, val_len])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "70c5fa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90080\n",
      "22519\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "414fb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(model, val_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = len(val_loader.dataset)\n",
    "    val_bar = tqdm(val_loader)\n",
    "    val_loss = 0\n",
    "    for step, (x, y) in enumerate(val_bar):\n",
    "        x = x.unsqueeze(dim=1).to(device).float()\n",
    "        y = y.to(device).long()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(dim=1)\n",
    "#             print(pred.shape, y.shape)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "        correct += torch.eq(pred, y).sum().float().item()\n",
    "    \n",
    "    return correct/total, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "87da7709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[1/30] loss: 1.245: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 36.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 122.49it/s]\n",
      "Train Epoch[2/30] loss: 1.181:   1%|█▋                                                                                                                                                  | 4/352 [00:00<00:09, 37.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.8661130600825969\n",
      "Validation loss decreased (inf --> 108.192708).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[2/30] loss: 1.166: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 38.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 122.30it/s]\n",
      "Train Epoch[3/30] loss: 1.128:   1%|█▋                                                                                                                                                  | 4/352 [00:00<00:09, 37.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9153603623606732\n",
      "Validation loss decreased (108.192708 --> 102.014379).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[3/30] loss: 1.131: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 38.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 122.57it/s]\n",
      "Train Epoch[4/30] loss: 1.096:   1%|█▋                                                                                                                                                  | 4/352 [00:00<00:09, 37.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9317021182112882\n",
      "Validation loss decreased (102.014379 --> 98.173956).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[4/30] loss: 1.117: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 38.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 122.32it/s]\n",
      "Train Epoch[5/30] loss: 1.083:   1%|█▋                                                                                                                                                  | 4/352 [00:00<00:09, 36.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9511079532838936\n",
      "Validation loss decreased (98.173956 --> 97.204791).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[5/30] loss: 1.097: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 38.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 122.81it/s]\n",
      "Train Epoch[6/30] loss: 1.085:   1%|█▋                                                                                                                                                  | 4/352 [00:00<00:09, 38.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9586571339757538\n",
      "Validation loss decreased (97.204791 --> 95.968716).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[6/30] loss: 1.091: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 38.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 108.41it/s]\n",
      "Train Epoch[7/30] loss: 1.079:   1%|█▋                                                                                                                                                  | 4/352 [00:00<00:09, 37.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9626537590479151\n",
      "Validation loss decreased (95.968716 --> 95.298266).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[7/30] loss: 1.085: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 38.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 122.27it/s]\n",
      "Train Epoch[8/30] loss: 1.080:   1%|█▋                                                                                                                                                  | 4/352 [00:00<00:09, 37.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9632310493361161\n",
      "Validation loss decreased (95.298266 --> 95.158148).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[8/30] loss: 1.084: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 38.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 122.67it/s]\n",
      "Train Epoch[9/30] loss: 1.074:   1%|█▋                                                                                                                                                  | 4/352 [00:00<00:09, 37.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9636751187885786\n",
      "Validation loss decreased (95.158148 --> 95.105627).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[9/30] loss: 1.084: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 38.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 121.66it/s]\n",
      "Train Epoch[10/30] loss: 1.071:   1%|█▋                                                                                                                                                 | 4/352 [00:00<00:09, 36.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9617656201429904\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[10/30] loss: 1.084: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 37.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 121.56it/s]\n",
      "Train Epoch[11/30] loss: 1.075:   1%|█▋                                                                                                                                                 | 4/352 [00:00<00:09, 36.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9639859674053022\n",
      "Validation loss decreased (95.105627 --> 95.021961).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[11/30] loss: 1.084: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 37.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 107.09it/s]\n",
      "Train Epoch[12/30] loss: 1.074:   1%|█▋                                                                                                                                                 | 4/352 [00:00<00:09, 37.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9634974910075936\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[12/30] loss: 1.084: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 37.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 121.23it/s]\n",
      "Train Epoch[13/30] loss: 1.071:   1%|█▋                                                                                                                                                 | 4/352 [00:00<00:09, 36.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9643412229672721\n",
      "Validation loss decreased (95.021961 --> 94.983420).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[13/30] loss: 1.084: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 37.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 121.54it/s]\n",
      "Train Epoch[14/30] loss: 1.073:   1%|█▋                                                                                                                                                 | 4/352 [00:00<00:09, 37.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9626981659931614\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[14/30] loss: 1.084: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 37.78it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 121.20it/s]\n",
      "Train Epoch[15/30] loss: 1.068:   1%|█▋                                                                                                                                                 | 4/352 [00:00<00:09, 37.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9623873173764377\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[15/30] loss: 1.083: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 37.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 120.75it/s]\n",
      "Train Epoch[16/30] loss: 1.067:   1%|█▋                                                                                                                                                 | 4/352 [00:00<00:09, 37.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9642524090767796\n",
      "Validation loss decreased (94.983420 --> 94.966859).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[16/30] loss: 1.083: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 37.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 106.82it/s]\n",
      "Train Epoch[17/30] loss: 1.071:   1%|█▋                                                                                                                                                 | 4/352 [00:00<00:09, 37.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.963941560460056\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[17/30] loss: 1.084: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 37.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 121.21it/s]\n",
      "Train Epoch[18/30] loss: 1.068:   1%|█▋                                                                                                                                                 | 4/352 [00:00<00:09, 37.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9590567964829699\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[18/30] loss: 1.084: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 352/352 [00:09<00:00, 37.67it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 88/88 [00:00<00:00, 120.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation ACC : 0.9630978285003775\n",
      "EarlyStopping counter: 3 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 3, verbose = True)\n",
    "\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    train_bar = tqdm(train_loader)\n",
    "    for step, (x,y) in enumerate(train_bar):\n",
    "        \n",
    "        x = x.unsqueeze(dim=1)\n",
    "#         print(\"input,\",x.shape, y.shape)\n",
    "        x, y = x.to(device).float(), y.to(device).long()\n",
    "        model.train()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss_list.append(loss)\n",
    "#         print(loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_bar.desc = \"Train Epoch[{}/{}] loss: {:.3f}\".format(ep+1, epochs, loss)\n",
    "        global_step +=1\n",
    "    \n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "    \n",
    "#     val_bar = tqdm(val_loader)\n",
    "#     for step, (x_, y_) in enumerate(val_bar):\n",
    "#         x_ = x_.unsqueeze(dim=1).to(device).float()\n",
    "#         y_pred = model(x_)\n",
    "#         y_ = y_.to(device).long()\n",
    "#         print(y_.shape, y_pred.shape)\n",
    "#         loss = criterion(y_pred, y_)\n",
    "        \n",
    "#         val_loss += loss.item()\n",
    "    \n",
    "    acc, val_loss = evaluate_acc(model, val_loader)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"./best.mdl\")\n",
    "    \n",
    "        \n",
    "    print(\"validation ACC :\",acc)\n",
    "    early_stopping(val_loss, model)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "59d804ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112599, 320]) torch.Size([112599])\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    pkl_path = \"./pickle/test/\"\n",
    "    data_x, data_y = [] ,[] \n",
    "    for pkl_file in os.listdir(pkl_path):\n",
    "        if os.path.isdir(pkl_path+pkl_file):\n",
    "            continue\n",
    "        with open(pkl_path+pkl_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            for i,d in enumerate(data[\"Beats\"]):\n",
    "                data_x.append(d)\n",
    "                data_y.append(data[\"symbol\"][i])\n",
    "#             print(torch.tensor(data[\"Beats\"]).shape)\n",
    "#             print(torch.tensor(data[\"symbol\"]).shape)\n",
    "    return torch.tensor(data_x), torch.tensor(data_y)\n",
    "\n",
    "x_test_data, y_test_data = load_data()\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test_data, y_test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7cdc3c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [00:00<00:00, 122.58it/s]\n"
     ]
    }
   ],
   "source": [
    "test_acc, _ = evaluate_acc(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "58aa248b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8926109565848349"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d722a",
   "metadata": {},
   "source": [
    "## Accuracy를 향상시키는 방법\n",
    "1. Denoising\n",
    "2. Augmentation\n",
    " * Scailing\n",
    " * Resampling\n",
    " * Clipping\n",
    "\n",
    "Augmentation 방법이 좀 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78bb85",
   "metadata": {},
   "source": [
    "### 10-Fold Corss Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1ae47d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7c1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
